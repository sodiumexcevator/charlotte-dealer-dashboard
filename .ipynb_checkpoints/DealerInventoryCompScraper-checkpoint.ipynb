{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a803e6ee",
   "metadata": {},
   "source": [
    "Use web scraping to pull the inv from 5 Local Dealers all of the same Make.(Keffer, Black, LakeNorman, Hendrick, and Stateline) From the data I get from the web scraping I want to answer four question:\n",
    "    1- Does the inventory match proportionally between these dealerships of the different vehicles from their makes?\n",
    "    2- How does the Amount of New Cars compare to that of used cars?\n",
    "    3- Whats the most popular make of the used car selections?\n",
    "    4- Whats the most popular on brand car?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f16436",
   "metadata": {},
   "source": [
    "page 1 https://www.kefferjeep.com/new-vehicles/?_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 2 https://www.kefferjeep.com/new-vehicles/?_p=1&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 3 https://www.kefferjeep.com/new-vehicles/?_p=2&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "\n",
    "lkn\n",
    "page 1 https://www.lakenormanchrysler.com/new-vehicles/?\n",
    "page 2 https://www.lakenormanchrysler.com/new-vehicles/?_p=1\n",
    "page 3 https://www.lakenormanchrysler.com/new-vehicles/?_p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d77c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\12522\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (4.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Requirement already satisfied: websockets in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04161006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84829cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Automatically download the correct driver\n",
    "service = Service(ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1258850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 20 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "Found 20 vehicle cards on page 29\n",
      "Scraping page 31...\n",
      "Found 20 vehicle cards on page 30\n",
      "Scraping page 32...\n",
      "Found 20 vehicle cards on page 31\n",
      "Scraping page 33...\n",
      "Found 20 vehicle cards on page 32\n",
      "Scraping page 34...\n",
      "Found 20 vehicle cards on page 33\n",
      "Scraping page 35...\n",
      "Found 20 vehicle cards on page 34\n",
      "Scraping page 36...\n",
      "Found 20 vehicle cards on page 35\n",
      "Scraping page 37...\n",
      "Found 20 vehicle cards on page 36\n",
      "Scraping page 38...\n",
      "Found 20 vehicle cards on page 37\n",
      "Scraping page 39...\n",
      "Found 20 vehicle cards on page 38\n",
      "Scraping page 40...\n",
      "Found 20 vehicle cards on page 39\n",
      "Scraping page 41...\n",
      "Found 20 vehicle cards on page 40\n",
      "Scraping page 42...\n",
      "Found 20 vehicle cards on page 41\n",
      "Scraping page 43...\n",
      "Found 20 vehicle cards on page 42\n",
      "Scraping page 44...\n",
      "Found 20 vehicle cards on page 43\n",
      "Scraping page 45...\n",
      "Found 20 vehicle cards on page 44\n",
      "Scraping page 46...\n",
      "Found 20 vehicle cards on page 45\n",
      "Scraping page 47...\n",
      "Found 20 vehicle cards on page 46\n",
      "Scraping page 48...\n",
      "Found 20 vehicle cards on page 47\n",
      "Scraping page 49...\n",
      "Found 20 vehicle cards on page 48\n",
      "Scraping page 50...\n",
      "Found 20 vehicle cards on page 49\n",
      "Scraping page 51...\n",
      "Found 20 vehicle cards on page 50\n",
      "Scraping page 52...\n",
      "Found 20 vehicle cards on page 51\n",
      "Scraping page 53...\n",
      "Found 20 vehicle cards on page 52\n",
      "Scraping page 54...\n",
      "Found 20 vehicle cards on page 53\n",
      "Scraping page 55...\n",
      "Found 20 vehicle cards on page 54\n",
      "Scraping page 56...\n",
      "Found 20 vehicle cards on page 55\n",
      "Scraping page 57...\n",
      "Found 20 vehicle cards on page 56\n",
      "Scraping page 58...\n",
      "Found 20 vehicle cards on page 57\n",
      "Scraping page 59...\n",
      "Found 20 vehicle cards on page 58\n",
      "Scraping page 60...\n",
      "Found 20 vehicle cards on page 59\n",
      "Scraping page 61...\n",
      "Found 20 vehicle cards on page 60\n",
      "Scraping page 62...\n",
      "Found 20 vehicle cards on page 61\n",
      "Scraping page 63...\n",
      "Found 20 vehicle cards on page 62\n",
      "Scraping page 64...\n",
      "Found 20 vehicle cards on page 63\n",
      "Scraping page 65...\n",
      "Found 20 vehicle cards on page 64\n",
      "Scraping page 66...\n",
      "Found 20 vehicle cards on page 65\n",
      "Scraping page 67...\n",
      "Found 20 vehicle cards on page 66\n",
      "Scraping page 68...\n",
      "Found 20 vehicle cards on page 67\n",
      "Scraping page 69...\n",
      "Found 20 vehicle cards on page 68\n",
      "Scraping page 70...\n",
      "Found 20 vehicle cards on page 69\n",
      "Scraping page 71...\n",
      "Found 20 vehicle cards on page 70\n",
      "Scraping page 72...\n",
      "Found 20 vehicle cards on page 71\n",
      "Scraping page 73...\n",
      "Found 20 vehicle cards on page 72\n",
      "Scraping page 74...\n",
      "Found 20 vehicle cards on page 73\n",
      "Scraping page 75...\n",
      "Found 20 vehicle cards on page 74\n",
      "Scraping page 76...\n",
      "Found 20 vehicle cards on page 75\n",
      "Scraping page 77...\n",
      "Found 20 vehicle cards on page 76\n",
      "Scraping page 78...\n",
      "Found 20 vehicle cards on page 77\n",
      "Scraping page 79...\n",
      "Found 12 vehicle cards on page 78\n",
      "Scraping page 80...\n",
      "No vehicle listings found or timed out on page 79. Ending.\n",
      "Scraped 1572 vehicles across 79 pages.\n"
     ]
    }
   ],
   "source": [
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.lakenormanchrysler.com/new-vehicles/?_p={page}\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_LKN_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8ece16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    1572\n",
       "Make-Model        1572\n",
       "Price             1534\n",
       "Vin               1572\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63357031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 19 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "No vehicle listings found or timed out on page 29. Ending.\n",
      "Scraped 579 vehicles across 29 pages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.kefferjeep.com/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_kef_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492a5d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    579\n",
       "Make-Model        579\n",
       "Price             572\n",
       "Vin               579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30ee67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 20 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "Found 20 vehicle cards on page 29\n",
      "Scraping page 31...\n",
      "Found 20 vehicle cards on page 30\n",
      "Scraping page 32...\n",
      "Found 20 vehicle cards on page 31\n",
      "Scraping page 33...\n",
      "Found 20 vehicle cards on page 32\n",
      "Scraping page 34...\n",
      "Found 20 vehicle cards on page 33\n",
      "Scraping page 35...\n",
      "Found 20 vehicle cards on page 34\n",
      "Scraping page 36...\n",
      "Found 20 vehicle cards on page 35\n",
      "Scraping page 37...\n",
      "Found 20 vehicle cards on page 36\n",
      "Scraping page 38...\n",
      "Found 20 vehicle cards on page 37\n",
      "Scraping page 39...\n",
      "Found 20 vehicle cards on page 38\n",
      "Scraping page 40...\n",
      "Found 20 vehicle cards on page 39\n",
      "Scraping page 41...\n",
      "Found 20 vehicle cards on page 40\n",
      "Scraping page 42...\n",
      "Found 20 vehicle cards on page 41\n",
      "Scraping page 43...\n",
      "Found 20 vehicle cards on page 42\n",
      "Scraping page 44...\n",
      "Found 20 vehicle cards on page 43\n",
      "Scraping page 45...\n",
      "Found 20 vehicle cards on page 44\n",
      "Scraping page 46...\n",
      "Found 20 vehicle cards on page 45\n",
      "Scraping page 47...\n",
      "Found 20 vehicle cards on page 46\n",
      "Scraping page 48...\n",
      "Found 20 vehicle cards on page 47\n",
      "Scraping page 49...\n",
      "Found 20 vehicle cards on page 48\n",
      "Scraping page 50...\n",
      "Found 20 vehicle cards on page 49\n",
      "Scraping page 51...\n",
      "Found 20 vehicle cards on page 50\n",
      "Scraping page 52...\n",
      "Found 20 vehicle cards on page 51\n",
      "Scraping page 53...\n",
      "Found 20 vehicle cards on page 52\n",
      "Scraping page 54...\n",
      "Found 20 vehicle cards on page 53\n",
      "Scraping page 55...\n",
      "Found 20 vehicle cards on page 54\n",
      "Scraping page 56...\n",
      "Found 20 vehicle cards on page 55\n",
      "Scraping page 57...\n",
      "Found 20 vehicle cards on page 56\n",
      "Scraping page 58...\n",
      "Found 20 vehicle cards on page 57\n",
      "Scraping page 59...\n",
      "Found 20 vehicle cards on page 58\n",
      "Scraping page 60...\n",
      "Found 20 vehicle cards on page 59\n",
      "Scraping page 61...\n",
      "Found 20 vehicle cards on page 60\n",
      "Scraping page 62...\n",
      "Found 20 vehicle cards on page 61\n",
      "Scraping page 63...\n",
      "Found 20 vehicle cards on page 62\n",
      "Scraping page 64...\n",
      "Found 20 vehicle cards on page 63\n",
      "Scraping page 65...\n",
      "Found 20 vehicle cards on page 64\n",
      "Scraping page 66...\n",
      "Found 20 vehicle cards on page 65\n",
      "Scraping page 67...\n",
      "Found 20 vehicle cards on page 66\n",
      "Scraping page 68...\n",
      "Found 20 vehicle cards on page 67\n",
      "Scraping page 69...\n",
      "Found 20 vehicle cards on page 68\n",
      "Scraping page 70...\n",
      "Found 20 vehicle cards on page 69\n",
      "Scraping page 71...\n",
      "Found 20 vehicle cards on page 70\n",
      "Scraping page 72...\n",
      "Found 20 vehicle cards on page 71\n",
      "Scraping page 73...\n",
      "Found 20 vehicle cards on page 72\n",
      "Scraping page 74...\n",
      "Found 20 vehicle cards on page 73\n",
      "Scraping page 75...\n",
      "Found 20 vehicle cards on page 74\n",
      "Scraping page 76...\n",
      "Found 20 vehicle cards on page 75\n",
      "Scraping page 77...\n",
      "Found 20 vehicle cards on page 76\n",
      "Scraping page 78...\n",
      "Found 20 vehicle cards on page 77\n",
      "Scraping page 79...\n",
      "Found 20 vehicle cards on page 78\n",
      "Scraping page 80...\n",
      "Found 20 vehicle cards on page 79\n",
      "Scraping page 81...\n",
      "Found 20 vehicle cards on page 80\n",
      "Scraping page 82...\n",
      "Found 20 vehicle cards on page 81\n",
      "Scraping page 83...\n",
      "Found 20 vehicle cards on page 82\n",
      "Scraping page 84...\n",
      "Found 20 vehicle cards on page 83\n",
      "Scraping page 85...\n",
      "Found 20 vehicle cards on page 84\n",
      "Scraping page 86...\n",
      "Found 20 vehicle cards on page 85\n",
      "Scraping page 87...\n",
      "Found 20 vehicle cards on page 86\n",
      "Scraping page 88...\n",
      "Found 20 vehicle cards on page 87\n",
      "Scraping page 89...\n",
      "Found 20 vehicle cards on page 88\n",
      "Scraping page 90...\n",
      "Found 20 vehicle cards on page 89\n",
      "Scraping page 91...\n",
      "Found 20 vehicle cards on page 90\n",
      "Scraping page 92...\n",
      "Found 20 vehicle cards on page 91\n",
      "Scraping page 93...\n",
      "Found 20 vehicle cards on page 92\n",
      "Scraping page 94...\n",
      "Found 20 vehicle cards on page 93\n",
      "Scraping page 95...\n",
      "Found 20 vehicle cards on page 94\n",
      "Scraping page 96...\n",
      "Found 20 vehicle cards on page 95\n",
      "Scraping page 97...\n",
      "Found 20 vehicle cards on page 96\n",
      "Scraping page 98...\n",
      "Found 20 vehicle cards on page 97\n",
      "Scraping page 99...\n",
      "Found 20 vehicle cards on page 98\n",
      "Scraping page 100...\n",
      "Found 20 vehicle cards on page 99\n",
      "Scraping page 101...\n",
      "Found 20 vehicle cards on page 100\n",
      "Scraping page 102...\n",
      "Found 20 vehicle cards on page 101\n",
      "Scraping page 103...\n",
      "Found 20 vehicle cards on page 102\n",
      "Scraping page 104...\n",
      "Found 20 vehicle cards on page 103\n",
      "Scraping page 105...\n",
      "Found 20 vehicle cards on page 104\n",
      "Scraping page 106...\n",
      "Found 20 vehicle cards on page 105\n",
      "Scraping page 107...\n",
      "Found 20 vehicle cards on page 106\n",
      "Scraping page 108...\n",
      "Found 20 vehicle cards on page 107\n",
      "Scraping page 109...\n",
      "Found 20 vehicle cards on page 108\n",
      "Scraping page 110...\n",
      "Found 20 vehicle cards on page 109\n",
      "Scraping page 111...\n",
      "Found 20 vehicle cards on page 110\n",
      "Scraping page 112...\n",
      "Found 20 vehicle cards on page 111\n",
      "Scraping page 113...\n",
      "Found 20 vehicle cards on page 112\n",
      "Scraping page 114...\n",
      "Found 20 vehicle cards on page 113\n",
      "Scraping page 115...\n",
      "Found 20 vehicle cards on page 114\n",
      "Scraping page 116...\n",
      "Found 20 vehicle cards on page 115\n",
      "Scraping page 117...\n",
      "Found 20 vehicle cards on page 116\n",
      "Scraping page 118...\n",
      "Found 6 vehicle cards on page 117\n",
      "Scraping page 119...\n",
      "No vehicle listings found or timed out on page 118. Ending.\n",
      "Scraped 2346 vehicles across 118 pages.\n"
     ]
    }
   ],
   "source": [
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.statelinechryslerjeepdodge.net/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_stateline_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2aaad5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    2346\n",
       "Make-Model        2346\n",
       "Price             2295\n",
       "Vin                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.gastoniadodge.com/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_gast_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
