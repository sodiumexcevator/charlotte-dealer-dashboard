{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b7494a",
   "metadata": {},
   "source": [
    "Use web scraping to pull the inv from 5 Local Dealers all of the same Make.(Keffer, Black, LakeNorman, Hendrick, and Stateline) From the data I get from the web scraping I want to answer four question:\n",
    "    1- Does the inventory match proportionally between these dealerships of the different vehicles from their makes?\n",
    "    2- How does the Amount of New Cars compare to that of used cars?\n",
    "    3- Whats the most popular make of the used car selections?\n",
    "    4- Whats the most popular on brand car?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6926bcf",
   "metadata": {},
   "source": [
    "page 1 https://www.kefferjeep.com/new-vehicles/?_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 2 https://www.kefferjeep.com/new-vehicles/?_p=1&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 3 https://www.kefferjeep.com/new-vehicles/?_p=2&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "\n",
    "lkn\n",
    "page 1 https://www.lakenormanchrysler.com/new-vehicles/?\n",
    "page 2 https://www.lakenormanchrysler.com/new-vehicles/?_p=1\n",
    "page 3 https://www.lakenormanchrysler.com/new-vehicles/?_p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e25fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\12522\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (4.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Requirement already satisfied: websockets in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1c9f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from selenium.common.exceptions import InvalidSessionIdException, WebDriverException\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d802d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Automatically download the correct driver\n",
    "service = Service(ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305eb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 20 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "Found 20 vehicle cards on page 29\n",
      "Scraping page 31...\n",
      "Found 20 vehicle cards on page 30\n",
      "Scraping page 32...\n",
      "Found 20 vehicle cards on page 31\n",
      "Scraping page 33...\n",
      "Found 20 vehicle cards on page 32\n",
      "Scraping page 34...\n",
      "Found 20 vehicle cards on page 33\n",
      "Scraping page 35...\n",
      "Found 20 vehicle cards on page 34\n",
      "Scraping page 36...\n",
      "Found 20 vehicle cards on page 35\n",
      "Scraping page 37...\n",
      "Found 20 vehicle cards on page 36\n",
      "Scraping page 38...\n",
      "Found 20 vehicle cards on page 37\n",
      "Scraping page 39...\n",
      "Found 20 vehicle cards on page 38\n",
      "Scraping page 40...\n",
      "Found 20 vehicle cards on page 39\n",
      "Scraping page 41...\n",
      "Found 20 vehicle cards on page 40\n",
      "Scraping page 42...\n",
      "Found 20 vehicle cards on page 41\n",
      "Scraping page 43...\n",
      "Found 20 vehicle cards on page 42\n",
      "Scraping page 44...\n",
      "Found 20 vehicle cards on page 43\n",
      "Scraping page 45...\n",
      "Found 20 vehicle cards on page 44\n",
      "Scraping page 46...\n",
      "Found 20 vehicle cards on page 45\n",
      "Scraping page 47...\n",
      "Found 20 vehicle cards on page 46\n",
      "Scraping page 48...\n",
      "Found 20 vehicle cards on page 47\n",
      "Scraping page 49...\n",
      "Found 20 vehicle cards on page 48\n",
      "Scraping page 50...\n",
      "Found 20 vehicle cards on page 49\n",
      "Scraping page 51...\n",
      "Found 20 vehicle cards on page 50\n",
      "Scraping page 52...\n",
      "Found 20 vehicle cards on page 51\n",
      "Scraping page 53...\n",
      "Found 20 vehicle cards on page 52\n",
      "Scraping page 54...\n",
      "Found 20 vehicle cards on page 53\n",
      "Scraping page 55...\n",
      "Found 20 vehicle cards on page 54\n",
      "Scraping page 56...\n",
      "Found 20 vehicle cards on page 55\n",
      "Scraping page 57...\n",
      "Found 20 vehicle cards on page 56\n",
      "Scraping page 58...\n",
      "Found 20 vehicle cards on page 57\n",
      "Scraping page 59...\n",
      "Found 20 vehicle cards on page 58\n",
      "Scraping page 60...\n",
      "Found 20 vehicle cards on page 59\n",
      "Scraping page 61...\n",
      "Found 20 vehicle cards on page 60\n",
      "Scraping page 62...\n",
      "Found 20 vehicle cards on page 61\n",
      "Scraping page 63...\n",
      "Found 20 vehicle cards on page 62\n",
      "Scraping page 64...\n",
      "Found 20 vehicle cards on page 63\n",
      "Scraping page 65...\n",
      "Found 20 vehicle cards on page 64\n",
      "Scraping page 66...\n",
      "Found 20 vehicle cards on page 65\n",
      "Scraping page 67...\n",
      "Found 20 vehicle cards on page 66\n",
      "Scraping page 68...\n",
      "Found 20 vehicle cards on page 67\n",
      "Scraping page 69...\n",
      "Found 20 vehicle cards on page 68\n",
      "Scraping page 70...\n",
      "Found 20 vehicle cards on page 69\n",
      "Scraping page 71...\n",
      "Found 20 vehicle cards on page 70\n",
      "Scraping page 72...\n",
      "Found 20 vehicle cards on page 71\n",
      "Scraping page 73...\n",
      "Found 20 vehicle cards on page 72\n",
      "Scraping page 74...\n",
      "Found 20 vehicle cards on page 73\n",
      "Scraping page 75...\n",
      "Found 20 vehicle cards on page 74\n",
      "Scraping page 76...\n",
      "Found 20 vehicle cards on page 75\n",
      "Scraping page 77...\n",
      "Found 20 vehicle cards on page 76\n",
      "Scraping page 78...\n",
      "Found 20 vehicle cards on page 77\n",
      "Scraping page 79...\n",
      "Found 12 vehicle cards on page 78\n",
      "Scraping page 80...\n",
      "No vehicle listings found or timed out on page 79. Ending.\n",
      "Scraped 1572 vehicles across 79 pages.\n"
     ]
    }
   ],
   "source": [
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.lakenormanchrysler.com/new-vehicles/?_p={page}\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_LKN_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ac358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    1572\n",
       "Make-Model        1572\n",
       "Price             1534\n",
       "Vin               1572\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9cd9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 19 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "No vehicle listings found or timed out on page 29. Ending.\n",
      "Scraped 579 vehicles across 29 pages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.kefferjeep.com/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_kef_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "043b2297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    579\n",
       "Make-Model        579\n",
       "Price             572\n",
       "Vin               579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61e42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=135.0.7049.85)\nStacktrace:\n\tGetHandleVerifier [0x00B48073+60707]\n\tGetHandleVerifier [0x00B480B4+60772]\n\t(No symbol) [0x00970683]\n\t(No symbol) [0x0094FBEE]\n\t(No symbol) [0x009E3C2E]\n\t(No symbol) [0x009FE129]\n\t(No symbol) [0x009DCE46]\n\t(No symbol) [0x009AC5D3]\n\t(No symbol) [0x009AD424]\n\tGetHandleVerifier [0x00D8BB53+2435075]\n\tGetHandleVerifier [0x00D870F3+2416035]\n\tGetHandleVerifier [0x00DA349C+2531660]\n\tGetHandleVerifier [0x00B5F145+155125]\n\tGetHandleVerifier [0x00B65AED+182173]\n\tGetHandleVerifier [0x00B4F948+91640]\n\tGetHandleVerifier [0x00B4FAF0+92064]\n\tGetHandleVerifier [0x00B3A5B0+4704]\n\tBaseThreadInitThunk [0x76645D49+25]\n\tRtlInitializeExceptionChain [0x777CCF0B+107]\n\tRtlGetAppContainerNamedObjectPath [0x777CCE91+561]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m elapsed \u001b[38;5;241m<\u001b[39m timeout:\n\u001b[1;32m---> 20\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     vehicle_cards \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhit-content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vehicle_cards:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\undetected_chromedriver\\__init__.py:806\u001b[0m, in \u001b[0;36mChrome.__getattribute__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 806\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:570\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    >>> print(driver.page_source)\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET_PAGE_SOURCE)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=135.0.7049.85)\nStacktrace:\n\tGetHandleVerifier [0x00B48073+60707]\n\tGetHandleVerifier [0x00B480B4+60772]\n\t(No symbol) [0x00970683]\n\t(No symbol) [0x0094FBEE]\n\t(No symbol) [0x009E3C2E]\n\t(No symbol) [0x009FE129]\n\t(No symbol) [0x009DCE46]\n\t(No symbol) [0x009AC5D3]\n\t(No symbol) [0x009AD424]\n\tGetHandleVerifier [0x00D8BB53+2435075]\n\tGetHandleVerifier [0x00D870F3+2416035]\n\tGetHandleVerifier [0x00DA349C+2531660]\n\tGetHandleVerifier [0x00B5F145+155125]\n\tGetHandleVerifier [0x00B65AED+182173]\n\tGetHandleVerifier [0x00B4F948+91640]\n\tGetHandleVerifier [0x00B4FAF0+92064]\n\tGetHandleVerifier [0x00B3A5B0+4704]\n\tBaseThreadInitThunk [0x76645D49+25]\n\tRtlInitializeExceptionChain [0x777CCF0B+107]\n\tRtlGetAppContainerNamedObjectPath [0x777CCE91+561]\n"
     ]
    }
   ],
   "source": [
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.statelinechryslerjeepdodge.net/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition/Year\": condition_year,\n",
    "                \"Make-Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_stateline_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b932fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    2346\n",
       "Make-Model        2346\n",
       "Price             2295\n",
       "Vin                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3672c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 (attempt 1)...\n",
      "Found 18 vehicle cards on page 0\n",
      "Scraping page 19 (attempt 1)...\n",
      "Found 18 vehicle cards on page 18\n",
      "Scraping page 37 (attempt 1)...\n",
      "Found 18 vehicle cards on page 36\n",
      "Scraping page 55 (attempt 1)...\n",
      "Found 18 vehicle cards on page 54\n",
      "Scraping page 73 (attempt 1)...\n",
      "Found 18 vehicle cards on page 72\n",
      "Scraping page 91 (attempt 1)...\n",
      "Found 18 vehicle cards on page 90\n",
      "Scraping page 109 (attempt 1)...\n",
      "Found 18 vehicle cards on page 108\n",
      "Scraping page 127 (attempt 1)...\n",
      "Found 18 vehicle cards on page 126\n",
      "Scraping page 145 (attempt 1)...\n",
      "Found 18 vehicle cards on page 144\n",
      "Scraping page 163 (attempt 1)...\n",
      "Found 18 vehicle cards on page 162\n",
      "Scraping page 181 (attempt 1)...\n",
      "Found 18 vehicle cards on page 180\n",
      "Scraping page 199 (attempt 1)...\n",
      "Found 18 vehicle cards on page 198\n",
      "Scraping page 217 (attempt 1)...\n",
      "Found 18 vehicle cards on page 216\n",
      "Scraping page 235 (attempt 1)...\n",
      "Found 18 vehicle cards on page 234\n",
      "Scraping page 253 (attempt 1)...\n",
      "Found 18 vehicle cards on page 252\n",
      "Scraping page 271 (attempt 1)...\n",
      "Found 18 vehicle cards on page 270\n",
      "Scraping page 289 (attempt 1)...\n",
      "Found 18 vehicle cards on page 288\n",
      "Scraping page 307 (attempt 1)...\n",
      "Found 18 vehicle cards on page 306\n",
      "Scraping page 325 (attempt 1)...\n",
      "Found 18 vehicle cards on page 324\n",
      "Scraping page 343 (attempt 1)...\n",
      "Found 18 vehicle cards on page 342\n",
      "Scraping page 361 (attempt 1)...\n",
      "Found 18 vehicle cards on page 360\n",
      "Scraping page 379 (attempt 1)...\n",
      "Found 18 vehicle cards on page 378\n",
      "Scraping page 397 (attempt 1)...\n",
      "Found 18 vehicle cards on page 396\n",
      "Scraping page 415 (attempt 1)...\n",
      "Found 18 vehicle cards on page 414\n",
      "Scraping page 433 (attempt 1)...\n",
      "Found 18 vehicle cards on page 432\n",
      "Scraping page 451 (attempt 1)...\n",
      "Found 18 vehicle cards on page 450\n",
      "Scraping page 469 (attempt 1)...\n",
      "Found 18 vehicle cards on page 468\n",
      "Scraping page 487 (attempt 1)...\n",
      "Found 18 vehicle cards on page 486\n",
      "Scraping page 505 (attempt 1)...\n",
      "Found 18 vehicle cards on page 504\n",
      "Scraping page 523 (attempt 1)...\n",
      "Found 18 vehicle cards on page 522\n",
      "Scraping page 541 (attempt 1)...\n",
      "Found 18 vehicle cards on page 540\n",
      "Scraping page 559 (attempt 1)...\n",
      "Found 18 vehicle cards on page 558\n",
      "Scraping page 577 (attempt 1)...\n",
      "Found 18 vehicle cards on page 576\n",
      "Scraping page 595 (attempt 1)...\n",
      "Found 18 vehicle cards on page 594\n",
      "Scraping page 613 (attempt 1)...\n",
      "Found 18 vehicle cards on page 612\n",
      "Scraping page 631 (attempt 1)...\n",
      "Found 18 vehicle cards on page 630\n",
      "Scraping page 649 (attempt 1)...\n",
      "Found 18 vehicle cards on page 648\n",
      "Scraping page 667 (attempt 1)...\n",
      "Found 18 vehicle cards on page 666\n",
      "Scraping page 685 (attempt 1)...\n",
      "Found 18 vehicle cards on page 684\n",
      "Scraping page 703 (attempt 1)...\n",
      "Found 18 vehicle cards on page 702\n",
      "Scraping page 721 (attempt 1)...\n",
      "Found 18 vehicle cards on page 720\n",
      "Scraping page 739 (attempt 1)...\n",
      "Found 18 vehicle cards on page 738\n",
      "Scraping page 757 (attempt 1)...\n",
      "Found 18 vehicle cards on page 756\n",
      "Scraping page 775 (attempt 1)...\n",
      "Found 18 vehicle cards on page 774\n",
      "Scraping page 793 (attempt 1)...\n",
      "Found 18 vehicle cards on page 792\n",
      "Scraping page 811 (attempt 1)...\n",
      "Found 18 vehicle cards on page 810\n",
      "Scraping page 829 (attempt 1)...\n",
      "Found 18 vehicle cards on page 828\n",
      "Scraping page 847 (attempt 1)...\n",
      "Found 18 vehicle cards on page 846\n",
      "Scraping page 865 (attempt 1)...\n",
      "Found 18 vehicle cards on page 864\n",
      "Scraping page 883 (attempt 1)...\n",
      "Found 18 vehicle cards on page 882\n",
      "Scraping page 901 (attempt 1)...\n",
      "Found 18 vehicle cards on page 900\n",
      "Scraping page 919 (attempt 1)...\n",
      "Found 18 vehicle cards on page 918\n",
      "Scraping page 937 (attempt 1)...\n",
      "Found 18 vehicle cards on page 936\n",
      "Scraping page 955 (attempt 1)...\n",
      "Found 18 vehicle cards on page 954\n",
      "Scraping page 973 (attempt 1)...\n",
      "Found 18 vehicle cards on page 972\n",
      "Scraping page 991 (attempt 1)...\n",
      "Found 18 vehicle cards on page 990\n",
      "Scraping page 1009 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1008\n",
      "Scraping page 1027 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1026\n",
      "Scraping page 1045 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1044\n",
      "Scraping page 1063 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1062\n",
      "Scraping page 1081 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1080\n",
      "Scraping page 1099 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1098\n",
      "Scraping page 1117 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1116\n",
      "Scraping page 1135 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1134\n",
      "Scraping page 1153 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1152\n",
      "Scraping page 1171 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1170\n",
      "Scraping page 1189 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1188\n",
      "Scraping page 1207 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1206\n",
      "Scraping page 1225 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1224\n",
      "Scraping page 1243 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1242\n",
      "Scraping page 1261 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1260\n",
      "Scraping page 1279 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1278\n",
      "Scraping page 1297 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1296\n",
      "Scraping page 1315 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1314\n",
      "Scraping page 1333 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1332\n",
      "Scraping page 1351 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1350\n",
      "Scraping page 1369 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1368\n",
      "Scraping page 1387 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1386\n",
      "Scraping page 1405 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1404\n",
      "Scraping page 1423 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1422\n",
      "Scraping page 1441 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1440\n",
      "Scraping page 1459 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1458\n",
      "Scraping page 1477 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1476\n",
      "Scraping page 1495 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1494\n",
      "Scraping page 1513 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1512\n",
      "Scraping page 1531 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1530\n",
      "Scraping page 1549 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1548\n",
      "Scraping page 1567 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1566\n",
      "Scraping page 1585 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1584\n",
      "Scraping page 1603 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1602\n",
      "Scraping page 1621 (attempt 1)...\n",
      "Found 17 vehicle cards on page 1620\n",
      "Scraping page 1639 (attempt 1)...\n",
      "No vehicle listings found or timed out on page 1638. Ending.\n",
      "Scraped 1637 vehicles across 79 pages.\n"
     ]
    }
   ],
   "source": [
    "def start_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "vehicles = []\n",
    "entry = 0\n",
    "max_retries = 3\n",
    "\n",
    "driver = start_driver()\n",
    "\n",
    "while True:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Scraping page {entry + 1} (attempt {retries + 1})...\")\n",
    "            url = f\"https://www.hendrickchryslerdodgejeepramofconcord.com/new-inventory/index.htm?start={entry}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            timeout = 20\n",
    "            poll_interval = 1\n",
    "            elapsed = 0\n",
    "\n",
    "            while elapsed < timeout:\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                vehicle_cards = soup.find_all(\"div\", class_=\"vehicle-card-details-container\")\n",
    "                if vehicle_cards:\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                elapsed += poll_interval\n",
    "            else:\n",
    "                print(f\"No vehicle listings found or timed out on page {entry}. Ending.\")\n",
    "                success = True  # Treat as success to exit retry loop, but end scrape\n",
    "                break\n",
    "\n",
    "            # Process vehicle cards\n",
    "            print(f\"Found {len(vehicle_cards)} vehicle cards on page {entry}\")\n",
    "            if not vehicle_cards:\n",
    "                print(\"No more vehicles found. Done.\")\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            for card in vehicle_cards:\n",
    "                try:\n",
    "                    condition_year_make_model = card.find(\"h2\", class_=\"vehicle-card-title mt-0 d-block mb-0 justify-content-between align-items-end h5 inv-type-new\")\n",
    "                    price = card.find(\"span\", class_=\"price-value\")\n",
    "\n",
    "                    vehicles.append({\n",
    "                        \"Condition/Year/make/model\": condition_year_make_model.get_text(strip=True) if condition_year else None,\n",
    "                        \"Price\": price.get_text(strip=True) if price else None,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing a card:\", e)\n",
    "\n",
    "            success = True\n",
    "            break  # Exit retry loop\n",
    "\n",
    "        except (InvalidSessionIdException, WebDriverException) as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Restarting browser...\")\n",
    "            driver = start_driver()\n",
    "            retries += 1\n",
    "            time.sleep(3)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Skipping page {page} after {max_retries} retries.\")\n",
    "        page += 1\n",
    "        continue\n",
    "\n",
    "    if not vehicle_cards:\n",
    "        break  # No more vehicles; end scraping\n",
    "\n",
    "    entry += 18\n",
    "\n",
    "# Quit browser and save data\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_hend_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01fc6da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year/make/model    1637\n",
       "Price                         273\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94542ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
