{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b7494a",
   "metadata": {},
   "source": [
    "Use web scraping to pull the inv from 5 Local Dealers all of the same Make.(Keffer, Black, LakeNorman, Hendrick, and Stateline) From the data I get from the web scraping I want to answer four question:\n",
    "    1- Does the inventory match proportionally between these dealerships of the different vehicles from their makes?\n",
    "    2- How does the Amount of New Cars compare to that of used cars?\n",
    "    3- Whats the most popular make of the used car selections?\n",
    "    4- Whats the most popular on brand car?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6926bcf",
   "metadata": {},
   "source": [
    "page 1 https://www.kefferjeep.com/new-vehicles/?_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 2 https://www.kefferjeep.com/new-vehicles/?_p=1&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "page 3 https://www.kefferjeep.com/new-vehicles/?_p=2&_dFR%5Blightning.isPolice%5D%5B0%5D=No\n",
    "\n",
    "lkn\n",
    "page 1 https://www.lakenormanchrysler.com/new-vehicles/?\n",
    "page 2 https://www.lakenormanchrysler.com/new-vehicles/?_p=1\n",
    "page 3 https://www.lakenormanchrysler.com/new-vehicles/?_p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e25fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\12522\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\12522\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\users\\12522\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (4.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Requirement already satisfied: websockets in c:\\users\\12522\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\12522\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\12522\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install undetected-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c9f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from selenium.common.exceptions import InvalidSessionIdException, WebDriverException\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d802d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Automatically download the correct driver\n",
    "service = Service(ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "305eb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 11 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "No vehicle listings found or timed out on page 13. Ending.\n",
      "Scraped 251 vehicles across 13 pages.\n"
     ]
    }
   ],
   "source": [
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.lakenormanchrysler.com/used-vehicles/?_p={page}&_dFR%5Blocation%5D%5B0%5D=Lake%2520Norman%2520Chrysler%2520Dodge%2520Jeep%2520Ram&_dFR%5Btype%5D%5B0%5D=Used&_dFR%5Btype%5D%5B1%5D=Certified%2520Used\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition_Year\": condition_year,\n",
    "                \"Make_Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin,\n",
    "                \"Date\": date.today()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_LKN_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ac358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    1572\n",
       "Make-Model        1572\n",
       "Price             1534\n",
       "Vin               1572\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cd9852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 15 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "No vehicle listings found or timed out on page 28. Ending.\n",
      "Scraped 555 vehicles across 28 pages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.kefferjeep.com/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition_Year\": condition_year,\n",
    "                \"Make_Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin,\n",
    "                \"Date\": date.today()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_kef_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "043b2297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    579\n",
       "Make-Model        579\n",
       "Price             572\n",
       "Vin               579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b61e42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 20 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "Found 20 vehicle cards on page 11\n",
      "Scraping page 13...\n",
      "Found 20 vehicle cards on page 12\n",
      "Scraping page 14...\n",
      "Found 20 vehicle cards on page 13\n",
      "Scraping page 15...\n",
      "Found 20 vehicle cards on page 14\n",
      "Scraping page 16...\n",
      "Found 20 vehicle cards on page 15\n",
      "Scraping page 17...\n",
      "Found 20 vehicle cards on page 16\n",
      "Scraping page 18...\n",
      "Found 20 vehicle cards on page 17\n",
      "Scraping page 19...\n",
      "Found 20 vehicle cards on page 18\n",
      "Scraping page 20...\n",
      "Found 20 vehicle cards on page 19\n",
      "Scraping page 21...\n",
      "Found 20 vehicle cards on page 20\n",
      "Scraping page 22...\n",
      "Found 20 vehicle cards on page 21\n",
      "Scraping page 23...\n",
      "Found 20 vehicle cards on page 22\n",
      "Scraping page 24...\n",
      "Found 20 vehicle cards on page 23\n",
      "Scraping page 25...\n",
      "Found 20 vehicle cards on page 24\n",
      "Scraping page 26...\n",
      "Found 20 vehicle cards on page 25\n",
      "Scraping page 27...\n",
      "Found 20 vehicle cards on page 26\n",
      "Scraping page 28...\n",
      "Found 20 vehicle cards on page 27\n",
      "Scraping page 29...\n",
      "Found 20 vehicle cards on page 28\n",
      "Scraping page 30...\n",
      "Found 20 vehicle cards on page 29\n",
      "Scraping page 31...\n",
      "Found 20 vehicle cards on page 30\n",
      "Scraping page 32...\n",
      "Found 20 vehicle cards on page 31\n",
      "Scraping page 33...\n",
      "Found 20 vehicle cards on page 32\n",
      "Scraping page 34...\n",
      "Found 20 vehicle cards on page 33\n",
      "Scraping page 35...\n",
      "Found 20 vehicle cards on page 34\n",
      "Scraping page 36...\n",
      "Found 20 vehicle cards on page 35\n",
      "Scraping page 37...\n",
      "Found 20 vehicle cards on page 36\n",
      "Scraping page 38...\n",
      "Found 20 vehicle cards on page 37\n",
      "Scraping page 39...\n",
      "Found 20 vehicle cards on page 38\n",
      "Scraping page 40...\n",
      "Found 20 vehicle cards on page 39\n",
      "Scraping page 41...\n",
      "Found 20 vehicle cards on page 40\n",
      "Scraping page 42...\n",
      "Found 20 vehicle cards on page 41\n",
      "Scraping page 43...\n",
      "Found 20 vehicle cards on page 42\n",
      "Scraping page 44...\n",
      "Found 20 vehicle cards on page 43\n",
      "Scraping page 45...\n",
      "Found 20 vehicle cards on page 44\n",
      "Scraping page 46...\n",
      "Found 20 vehicle cards on page 45\n",
      "Scraping page 47...\n",
      "Found 20 vehicle cards on page 46\n",
      "Scraping page 48...\n",
      "Found 20 vehicle cards on page 47\n",
      "Scraping page 49...\n",
      "Found 20 vehicle cards on page 48\n",
      "Scraping page 50...\n",
      "Found 20 vehicle cards on page 49\n",
      "Scraping page 51...\n",
      "Found 20 vehicle cards on page 50\n",
      "Scraping page 52...\n",
      "Found 20 vehicle cards on page 51\n",
      "Scraping page 53...\n",
      "Found 20 vehicle cards on page 52\n",
      "Scraping page 54...\n",
      "Found 20 vehicle cards on page 53\n",
      "Scraping page 55...\n",
      "Found 20 vehicle cards on page 54\n",
      "Scraping page 56...\n",
      "Found 20 vehicle cards on page 55\n",
      "Scraping page 57...\n",
      "Found 20 vehicle cards on page 56\n",
      "Scraping page 58...\n",
      "Found 20 vehicle cards on page 57\n",
      "Scraping page 59...\n",
      "Found 20 vehicle cards on page 58\n",
      "Scraping page 60...\n",
      "Found 20 vehicle cards on page 59\n",
      "Scraping page 61...\n",
      "Found 20 vehicle cards on page 60\n",
      "Scraping page 62...\n",
      "Found 20 vehicle cards on page 61\n",
      "Scraping page 63...\n",
      "Found 20 vehicle cards on page 62\n",
      "Scraping page 64...\n",
      "Found 20 vehicle cards on page 63\n",
      "Scraping page 65...\n",
      "Found 20 vehicle cards on page 64\n",
      "Scraping page 66...\n",
      "Found 20 vehicle cards on page 65\n",
      "Scraping page 67...\n",
      "Found 20 vehicle cards on page 66\n",
      "Scraping page 68...\n",
      "Found 20 vehicle cards on page 67\n",
      "Scraping page 69...\n",
      "Found 20 vehicle cards on page 68\n",
      "Scraping page 70...\n",
      "Found 20 vehicle cards on page 69\n",
      "Scraping page 71...\n",
      "Found 20 vehicle cards on page 70\n",
      "Scraping page 72...\n",
      "Found 20 vehicle cards on page 71\n",
      "Scraping page 73...\n",
      "Found 20 vehicle cards on page 72\n",
      "Scraping page 74...\n",
      "Found 20 vehicle cards on page 73\n",
      "Scraping page 75...\n",
      "Found 20 vehicle cards on page 74\n",
      "Scraping page 76...\n",
      "Found 20 vehicle cards on page 75\n",
      "Scraping page 77...\n",
      "Found 20 vehicle cards on page 76\n",
      "Scraping page 78...\n",
      "Found 20 vehicle cards on page 77\n",
      "Scraping page 79...\n",
      "Found 20 vehicle cards on page 78\n",
      "Scraping page 80...\n",
      "Found 20 vehicle cards on page 79\n",
      "Scraping page 81...\n",
      "Found 20 vehicle cards on page 80\n",
      "Scraping page 82...\n",
      "Found 20 vehicle cards on page 81\n",
      "Scraping page 83...\n",
      "Found 20 vehicle cards on page 82\n",
      "Scraping page 84...\n",
      "Found 20 vehicle cards on page 83\n",
      "Scraping page 85...\n",
      "Found 20 vehicle cards on page 84\n",
      "Scraping page 86...\n",
      "Found 20 vehicle cards on page 85\n",
      "Scraping page 87...\n",
      "Found 20 vehicle cards on page 86\n",
      "Scraping page 88...\n",
      "Found 20 vehicle cards on page 87\n",
      "Scraping page 89...\n",
      "Found 20 vehicle cards on page 88\n",
      "Scraping page 90...\n",
      "Found 20 vehicle cards on page 89\n",
      "Scraping page 91...\n",
      "Found 20 vehicle cards on page 90\n",
      "Scraping page 92...\n",
      "Found 20 vehicle cards on page 91\n",
      "Scraping page 93...\n",
      "Found 20 vehicle cards on page 92\n",
      "Scraping page 94...\n",
      "Found 20 vehicle cards on page 93\n",
      "Scraping page 95...\n",
      "Found 20 vehicle cards on page 94\n",
      "Scraping page 96...\n",
      "Found 20 vehicle cards on page 95\n",
      "Scraping page 97...\n",
      "Found 20 vehicle cards on page 96\n",
      "Scraping page 98...\n",
      "Found 20 vehicle cards on page 97\n",
      "Scraping page 99...\n",
      "Found 20 vehicle cards on page 98\n",
      "Scraping page 100...\n",
      "Found 20 vehicle cards on page 99\n",
      "Scraping page 101...\n",
      "Found 20 vehicle cards on page 100\n",
      "Scraping page 102...\n",
      "Found 20 vehicle cards on page 101\n",
      "Scraping page 103...\n",
      "Found 20 vehicle cards on page 102\n",
      "Scraping page 104...\n",
      "Found 20 vehicle cards on page 103\n",
      "Scraping page 105...\n",
      "Found 20 vehicle cards on page 104\n",
      "Scraping page 106...\n",
      "Found 20 vehicle cards on page 105\n",
      "Scraping page 107...\n",
      "Found 20 vehicle cards on page 106\n",
      "Scraping page 108...\n",
      "Found 20 vehicle cards on page 107\n",
      "Scraping page 109...\n",
      "Found 20 vehicle cards on page 108\n",
      "Scraping page 110...\n",
      "Found 20 vehicle cards on page 109\n",
      "Scraping page 111...\n",
      "Found 20 vehicle cards on page 110\n",
      "Scraping page 112...\n",
      "Found 20 vehicle cards on page 111\n",
      "Scraping page 113...\n",
      "Found 20 vehicle cards on page 112\n",
      "Scraping page 114...\n",
      "Found 20 vehicle cards on page 113\n",
      "Scraping page 115...\n",
      "Found 20 vehicle cards on page 114\n",
      "Scraping page 116...\n",
      "Found 20 vehicle cards on page 115\n",
      "Scraping page 117...\n",
      "Found 11 vehicle cards on page 116\n",
      "Scraping page 118...\n",
      "No vehicle listings found or timed out on page 117. Ending.\n",
      "Scraped 2331 vehicles across 117 pages.\n"
     ]
    }
   ],
   "source": [
    "# Recreate ChromeOptions to avoid reuse error\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Now this won't throw an error\n",
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.statelinechryslerjeepdodge.net/new-vehicles/?_p={page}&_dFR%5Blightning.isPolice%5D%5B0%5D=No\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition_Year\": condition_year,\n",
    "                \"Make_Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin,\n",
    "                \"Date\": date.today()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_stateline_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b932fa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year    2346\n",
       "Make-Model        2346\n",
       "Price             2295\n",
       "Vin                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3672c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 (attempt 1)...\n",
      "Found 18 vehicle cards on page 0\n",
      "Scraping page 19 (attempt 1)...\n",
      "Found 18 vehicle cards on page 18\n",
      "Scraping page 37 (attempt 1)...\n",
      "Found 18 vehicle cards on page 36\n",
      "Scraping page 55 (attempt 1)...\n",
      "Found 18 vehicle cards on page 54\n",
      "Scraping page 73 (attempt 1)...\n",
      "Found 18 vehicle cards on page 72\n",
      "Scraping page 91 (attempt 1)...\n",
      "Found 18 vehicle cards on page 90\n",
      "Scraping page 109 (attempt 1)...\n",
      "Found 18 vehicle cards on page 108\n",
      "Scraping page 127 (attempt 1)...\n",
      "Found 18 vehicle cards on page 126\n",
      "Scraping page 145 (attempt 1)...\n",
      "Found 18 vehicle cards on page 144\n",
      "Scraping page 163 (attempt 1)...\n",
      "Found 18 vehicle cards on page 162\n",
      "Scraping page 181 (attempt 1)...\n",
      "Found 18 vehicle cards on page 180\n",
      "Scraping page 199 (attempt 1)...\n",
      "Found 18 vehicle cards on page 198\n",
      "Scraping page 217 (attempt 1)...\n",
      "Found 18 vehicle cards on page 216\n",
      "Scraping page 235 (attempt 1)...\n",
      "Found 18 vehicle cards on page 234\n",
      "Scraping page 253 (attempt 1)...\n",
      "Found 18 vehicle cards on page 252\n",
      "Scraping page 271 (attempt 1)...\n",
      "Found 18 vehicle cards on page 270\n",
      "Scraping page 289 (attempt 1)...\n",
      "Found 18 vehicle cards on page 288\n",
      "Scraping page 307 (attempt 1)...\n",
      "Found 18 vehicle cards on page 306\n",
      "Scraping page 325 (attempt 1)...\n",
      "Found 18 vehicle cards on page 324\n",
      "Scraping page 343 (attempt 1)...\n",
      "Found 18 vehicle cards on page 342\n",
      "Scraping page 361 (attempt 1)...\n",
      "Found 18 vehicle cards on page 360\n",
      "Scraping page 379 (attempt 1)...\n",
      "Found 18 vehicle cards on page 378\n",
      "Scraping page 397 (attempt 1)...\n",
      "Found 18 vehicle cards on page 396\n",
      "Scraping page 415 (attempt 1)...\n",
      "Found 18 vehicle cards on page 414\n",
      "Scraping page 433 (attempt 1)...\n",
      "Found 18 vehicle cards on page 432\n",
      "Scraping page 451 (attempt 1)...\n",
      "Found 18 vehicle cards on page 450\n",
      "Scraping page 469 (attempt 1)...\n",
      "Found 18 vehicle cards on page 468\n",
      "Scraping page 487 (attempt 1)...\n",
      "Found 18 vehicle cards on page 486\n",
      "Scraping page 505 (attempt 1)...\n",
      "Found 18 vehicle cards on page 504\n",
      "Scraping page 523 (attempt 1)...\n",
      "Found 18 vehicle cards on page 522\n",
      "Scraping page 541 (attempt 1)...\n",
      "Found 18 vehicle cards on page 540\n",
      "Scraping page 559 (attempt 1)...\n",
      "Found 18 vehicle cards on page 558\n",
      "Scraping page 577 (attempt 1)...\n",
      "Found 18 vehicle cards on page 576\n",
      "Scraping page 595 (attempt 1)...\n",
      "Found 18 vehicle cards on page 594\n",
      "Scraping page 613 (attempt 1)...\n",
      "Found 18 vehicle cards on page 612\n",
      "Scraping page 631 (attempt 1)...\n",
      "Found 18 vehicle cards on page 630\n",
      "Scraping page 649 (attempt 1)...\n",
      "Found 18 vehicle cards on page 648\n",
      "Scraping page 667 (attempt 1)...\n",
      "Found 18 vehicle cards on page 666\n",
      "Scraping page 685 (attempt 1)...\n",
      "Found 18 vehicle cards on page 684\n",
      "Scraping page 703 (attempt 1)...\n",
      "Found 18 vehicle cards on page 702\n",
      "Scraping page 721 (attempt 1)...\n",
      "Found 18 vehicle cards on page 720\n",
      "Scraping page 739 (attempt 1)...\n",
      "Found 18 vehicle cards on page 738\n",
      "Scraping page 757 (attempt 1)...\n",
      "Found 18 vehicle cards on page 756\n",
      "Scraping page 775 (attempt 1)...\n",
      "Found 18 vehicle cards on page 774\n",
      "Scraping page 793 (attempt 1)...\n",
      "Found 18 vehicle cards on page 792\n",
      "Scraping page 811 (attempt 1)...\n",
      "Found 18 vehicle cards on page 810\n",
      "Scraping page 829 (attempt 1)...\n",
      "Found 18 vehicle cards on page 828\n",
      "Scraping page 847 (attempt 1)...\n",
      "Found 18 vehicle cards on page 846\n",
      "Scraping page 865 (attempt 1)...\n",
      "Found 18 vehicle cards on page 864\n",
      "Scraping page 883 (attempt 1)...\n",
      "Found 18 vehicle cards on page 882\n",
      "Scraping page 901 (attempt 1)...\n",
      "Found 18 vehicle cards on page 900\n",
      "Scraping page 919 (attempt 1)...\n",
      "Found 18 vehicle cards on page 918\n",
      "Scraping page 937 (attempt 1)...\n",
      "Found 18 vehicle cards on page 936\n",
      "Scraping page 955 (attempt 1)...\n",
      "Found 18 vehicle cards on page 954\n",
      "Scraping page 973 (attempt 1)...\n",
      "Found 18 vehicle cards on page 972\n",
      "Scraping page 991 (attempt 1)...\n",
      "Found 18 vehicle cards on page 990\n",
      "Scraping page 1009 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1008\n",
      "Scraping page 1027 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1026\n",
      "Scraping page 1045 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1044\n",
      "Scraping page 1063 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1062\n",
      "Scraping page 1081 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1080\n",
      "Scraping page 1099 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1098\n",
      "Scraping page 1117 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1116\n",
      "Scraping page 1135 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1134\n",
      "Scraping page 1153 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1152\n",
      "Scraping page 1171 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1170\n",
      "Scraping page 1189 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1188\n",
      "Scraping page 1207 (attempt 1)...\n",
      "Found 18 vehicle cards on page 1206\n",
      "Scraping page 1225 (attempt 1)...\n",
      "Found 9 vehicle cards on page 1224\n",
      "Scraping page 1243 (attempt 1)...\n",
      "No vehicle listings found or timed out on page 1242. Ending.\n",
      "Scraped 1233 vehicles across 117 pages.\n"
     ]
    }
   ],
   "source": [
    "def start_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "vehicles = []\n",
    "entry = 0\n",
    "max_retries = 3\n",
    "\n",
    "driver = start_driver()\n",
    "\n",
    "while True:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Scraping page {entry + 1} (attempt {retries + 1})...\")\n",
    "            url = f\"https://www.hendrickchryslerdodgejeepramofconcord.com/new-inventory/index.htm?start={entry}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            timeout = 20\n",
    "            poll_interval = 1\n",
    "            elapsed = 0\n",
    "\n",
    "            while elapsed < timeout:\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                vehicle_cards = soup.find_all(\"div\", class_=\"vehicle-card-details-container\")\n",
    "                if vehicle_cards:\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                elapsed += poll_interval\n",
    "            else:\n",
    "                print(f\"No vehicle listings found or timed out on page {entry}. Ending.\")\n",
    "                success = True  # Treat as success to exit retry loop, but end scrape\n",
    "                break\n",
    "\n",
    "            # Process vehicle cards\n",
    "            print(f\"Found {len(vehicle_cards)} vehicle cards on page {entry}\")\n",
    "            if not vehicle_cards:\n",
    "                print(\"No more vehicles found. Done.\")\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            for card in vehicle_cards:\n",
    "                try:\n",
    "                    condition_year_make_model = card.find(\"h2\", class_=\"vehicle-card-title mt-0 d-block mb-0 justify-content-between align-items-end h5 inv-type-new\")\n",
    "                    price = card.find(\"span\", class_=\"price-value\")\n",
    "\n",
    "                    vehicles.append({\n",
    "                        \"Condition_Year_Make_Model\": condition_year_make_model.get_text(strip=True) if condition_year_make_model else None,\n",
    "                        \"Price\": price.get_text(strip=True) if price else None,\n",
    "                        \"Date\": date.today()\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing a card:\", e)\n",
    "\n",
    "            success = True\n",
    "            break  # Exit retry loop\n",
    "\n",
    "        except (InvalidSessionIdException, WebDriverException) as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Restarting browser...\")\n",
    "            driver = start_driver()\n",
    "            retries += 1\n",
    "            time.sleep(3)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Skipping page {page} after {max_retries} retries.\")\n",
    "        page += 1\n",
    "        continue\n",
    "\n",
    "    if not vehicle_cards:\n",
    "        break  # No more vehicles; end scraping\n",
    "\n",
    "    entry += 18\n",
    "\n",
    "# Quit browser and save data\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_hend_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01fc6da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition/Year/make/model    1637\n",
       "Price                         273\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225556c-bf50-4caf-b64d-648f10e53df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9b9597-d650-46ba-ae99-f8181c4f6c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 20 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "Found 20 vehicle cards on page 3\n",
      "Scraping page 5...\n",
      "Found 20 vehicle cards on page 4\n",
      "Scraping page 6...\n",
      "Found 20 vehicle cards on page 5\n",
      "Scraping page 7...\n",
      "Found 20 vehicle cards on page 6\n",
      "Scraping page 8...\n",
      "Found 20 vehicle cards on page 7\n",
      "Scraping page 9...\n",
      "Found 20 vehicle cards on page 8\n",
      "Scraping page 10...\n",
      "Found 20 vehicle cards on page 9\n",
      "Scraping page 11...\n",
      "Found 2 vehicle cards on page 10\n",
      "Scraping page 12...\n",
      "No vehicle listings found or timed out on page 11. Ending.\n",
      "Scraped 202 vehicles across 11 pages.\n"
     ]
    }
   ],
   "source": [
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.toyotaofnorthcharlotte.com/used-vehicles/?_p={page}&_dFR%5Btype%5D%5B0%5D=Used&_dFR%5Btype%5D%5B1%5D=Certified%2520Used\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            year_make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            year_make_model = year_make_model_el.get_text(strip=True) if year_make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition\": \"Used\",\n",
    "                \"Year_Make_Model\": year_make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin,\n",
    "                \"Date\": date.today()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_NcharToyo_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4c65c3-f865-4e32-be4b-28a37544eed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition          202\n",
       "Year_Make_Model    202\n",
       "Price              192\n",
       "Vin                202\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94542ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 20 vehicle cards on page 0\n",
      "Scraping page 2...\n",
      "Found 20 vehicle cards on page 1\n",
      "Scraping page 3...\n",
      "Found 19 vehicle cards on page 2\n",
      "Scraping page 4...\n",
      "No vehicle listings found or timed out on page 3. Ending.\n",
      "Scraped 59 vehicles across 3 pages.\n"
     ]
    }
   ],
   "source": [
    "driver = uc.Chrome(options=options)\n",
    "vehicles = []\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page+1}...\")\n",
    "    url = f\"https://www.modernnissanoflakenorman.com/used-vehicles/?_p={page}&_dFR%5BLocation%5D%5B0%5D=Modern%2520Nissan%2520of%2520Lake%2520Norman&_dFR%5Btype%5D%5B0%5D=Pre-Owned&_dFR%5Btype%5D%5B1%5D=Certified%2520Pre-Owned\"\n",
    "    driver.get(url)\n",
    "    # Wait until vehicle cards are present or timeout after 20 seconds\n",
    "    timeout = 20\n",
    "    poll_interval = 1\n",
    "    elapsed = 0\n",
    "\n",
    "    while elapsed < timeout:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "        if vehicle_cards:\n",
    "            break\n",
    "        time.sleep(poll_interval)\n",
    "        elapsed += poll_interval\n",
    "    else:\n",
    "        print(f\"No vehicle listings found or timed out on page {page}. Ending.\")\n",
    "        break\n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    vehicle_cards = soup.find_all(\"div\", class_=\"hit-content\")\n",
    "    print(f\"Found {len(vehicle_cards)} vehicle cards on page {page}\")\n",
    "    \n",
    "    # If no vehicles are found, break the loop\n",
    "    if not vehicle_cards:\n",
    "        print(\"No more vehicles found. Done.\")\n",
    "        break\n",
    "\n",
    "    for card in vehicle_cards:\n",
    "        try:\n",
    "            # Extract elements (not strings yet)\n",
    "            condition_year_el = card.find(\"span\", class_=\"title-top\")\n",
    "            make_model_el = card.find(\"span\", class_=\"title-bottom\")\n",
    "            price_el = card.find(\"span\", class_=\"price\")\n",
    "            vin_el = card.find(\"div\", class_=\"vin-row\")\n",
    "\n",
    "            # Now safely extract text only if the element exists\n",
    "            condition_year = condition_year_el.get_text(strip=True) if condition_year_el else None\n",
    "            make_model = make_model_el.get_text(strip=True) if make_model_el else None\n",
    "            price = price_el.get_text(strip=True) if price_el else None\n",
    "            vin = vin_el.get_text(strip=True) if vin_el else None\n",
    "\n",
    "            vehicles.append({\n",
    "                \"Condition_Year\": condition_year,\n",
    "                \"Make_Model\": make_model,\n",
    "                \"Price\": price,\n",
    "                \"Vin\": vin,\n",
    "                \"Date\": date.today()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing a card:\", e)\n",
    "\n",
    "    page += 1  # Go to next page\n",
    "\n",
    "# Quit browser and save data\n",
    "driver.quit()\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_mod_Nissan_LKN_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b1b8d4-148f-4e8f-83d5-3f2d1ae25f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 2 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 3 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 4 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 5 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 6 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 7 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 8 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 9 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 10 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 11 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 12 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 13 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 14 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 15 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 16 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 17 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 18 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 19 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 20 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 21 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 22 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 23 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 24 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 25 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 26 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 27 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 28 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 29 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 30 (attempt 1)...\n",
      "Loopback detected on page 29 (stock #None already seen). Stopping.\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraped 360 vehicles across 29 pages.\n"
     ]
    }
   ],
   "source": [
    "def start_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "vehicles = []\n",
    "entry = 0\n",
    "max_retries = 3\n",
    "page = 0\n",
    "seen_first_stocks = set()\n",
    "driver = start_driver()\n",
    "stop_scraping = False\n",
    "while True:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Scraping page {page + 1} (attempt {retries + 1})...\")\n",
    "            url = f\"https://www.lakenormanhyundai.com/search/cornelius-nc/?cy=28031&la=our_inventory&p={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            timeout = 20\n",
    "            poll_interval = 1\n",
    "            elapsed = 0\n",
    "\n",
    "            while elapsed < timeout:\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                vehicle_cards = soup.find_all(\"div\", class_=\"vehicle_item\")\n",
    "                # Check for loopback by comparing first stock number\n",
    "                first_card = vehicle_cards[0]\n",
    "                first_stock = first_card.find(\"span\", class_=\"single-line-details__item\")\n",
    "                first_stock_text = first_stock.get_text(strip=True) if first_stock else None\n",
    "\n",
    "                if first_stock_text in seen_first_stocks:\n",
    "                    print(f\"Loopback detected on page {page} (stock #{first_stock_text} already seen). Stopping.\")\n",
    "                    stop_scraping = True\n",
    "                    success = True\n",
    "                    break  # Break the retry loop\n",
    "\n",
    "                seen_first_stocks.add(first_stock_text)\n",
    "                if vehicle_cards:\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                elapsed += poll_interval\n",
    "            else:\n",
    "                print(f\"No vehicle listings found or timed out on page {entry}. Ending.\")\n",
    "                success = True  # Treat as success to exit retry loop, but end scrape\n",
    "                break\n",
    "\n",
    "            # Process vehicle cards\n",
    "            print(f\"Found {len(vehicle_cards)} vehicle cards on page {entry}\")\n",
    "            if not vehicle_cards:\n",
    "                print(\"No more vehicles found. Done.\")\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            for card in vehicle_cards:\n",
    "                try:\n",
    "                    condition=card.find(\"span\", class_=\"pipe_separated__span pipe_separated__span--Type thm-lighter_text_color\")\n",
    "                    year=card.find(\"span\", class_=\"pipe_separated__span pipe_separated__span--Year thm-lighter_text_color\")\n",
    "                    make_model = card.find(\"h2\", class_=\"vehicle_title\")\n",
    "                    price = card.find(\"dd\", class_=\"vehicle_price  \")\n",
    "                    stock = card.find(\"span\", class_=\"single-line-details__item\")\n",
    "\n",
    "                    vehicles.append({\n",
    "                        \"Condition\": condition.get_text(strip=True) if condition else None,\n",
    "                        \"Year\": year.get_text(strip=True) if year else None,\n",
    "                        \"Make_Model\": make_model.get_text(strip=True) if make_model else None,\n",
    "                        \"Price\": price.get_text(strip=True) if price else None,\n",
    "                        \"Stock\": stock.get_text(strip=True) if stock else None,\n",
    "                        \"Date\": date.today()\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing a card:\", e)\n",
    "\n",
    "            success = True\n",
    "            break  # Exit retry loop\n",
    "\n",
    "        except (InvalidSessionIdException, WebDriverException) as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Restarting browser...\")\n",
    "            driver = start_driver()\n",
    "            retries += 1\n",
    "            time.sleep(3)\n",
    "    if stop_scraping:\n",
    "        break  # Exit outer while True loop\n",
    "    if not success:\n",
    "        print(f\"Skipping page {page} after {max_retries} retries.\")\n",
    "        page += 1\n",
    "        continue\n",
    "\n",
    "    if not vehicle_cards:\n",
    "        break  # No more vehicles; end scraping\n",
    "\n",
    "    page += 1\n",
    "\n",
    "# Quit browser and save data\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_LKNhyun_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b9f4c9-baf7-40bf-8bf3-ef0acf6bdb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 3 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 4 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 5 (attempt 1)...\n",
      "Found 12 vehicle cards on page 0\n",
      "Scraping page 6 (attempt 1)...\n",
      "Found 9 vehicle cards on page 0\n",
      "Scraping page 7 (attempt 1)...\n",
      "No vehicle listings found or timed out on page 0. Ending.\n",
      "Scraped 57 vehicles across 6 pages.\n"
     ]
    }
   ],
   "source": [
    "def start_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "vehicles = []\n",
    "entry = 0\n",
    "max_retries = 3\n",
    "page = 1\n",
    "seen_first_stocks = set()\n",
    "driver = start_driver()\n",
    "stop_scraping = False\n",
    "while True:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Scraping page {page + 1} (attempt {retries + 1})...\")\n",
    "            url = f\"https://www.lakenormaninfiniti.com/our-used-vehicles-charlotte-nc.html?pt={page}#\"\n",
    "            driver.get(url)\n",
    "\n",
    "            timeout = 20\n",
    "            poll_interval = 1\n",
    "            elapsed = 0\n",
    "\n",
    "            while elapsed < timeout:\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                vehicle_cards = soup.find_all(\"div\", class_=\"vehicle-overview save-heart__srp-card-overview\")\n",
    "                if vehicle_cards:\n",
    "                    # Check for loopback by comparing first stock number\n",
    "                    first_card = vehicle_cards[0]\n",
    "                    first_stock = first_card.find(\"span\", class_=\"vehicle-identifiers__value\")\n",
    "                    first_stock_text = first_stock.get_text(strip=True) if first_stock else None\n",
    "\n",
    "                    if first_stock_text in seen_first_stocks:\n",
    "                        print(f\"Loopback detected on page {page} (stock #{first_stock_text} already seen). Stopping.\")\n",
    "                        stop_scraping = True\n",
    "                        success = True\n",
    "                        break  # Break the retry loop\n",
    "                    seen_first_stocks.add(first_stock_text)\n",
    "                    break\n",
    "                time.sleep(poll_interval)\n",
    "                elapsed += poll_interval\n",
    "            else:\n",
    "                print(f\"No vehicle listings found or timed out on page {entry}. Ending.\")\n",
    "                success = True  # Treat as success to exit retry loop, but end scrape\n",
    "                break\n",
    "\n",
    "            # Process vehicle cards\n",
    "            print(f\"Found {len(vehicle_cards)} vehicle cards on page {entry}\")\n",
    "            if not vehicle_cards:\n",
    "                print(\"No more vehicles found. Done.\")\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            for card in vehicle_cards:\n",
    "                try:\n",
    "                        year=card.find(\"span\", class_=\"vehicle-title__year\")\n",
    "                        make_model = card.find(\"span\", class_=\"vehicle-title__make-model\")\n",
    "                        trim = card.find(\"span\", class_=\"vehicle-title__trim\")\n",
    "                        price = card.find(\"dd\", class_=\"vehicle_price  \")\n",
    "                        vin = card.find(\"span\", class_=\"vehicle-identifiers__value\")\n",
    "                    \n",
    "                        vehicles.append({\n",
    "                            \"Condition\": \"Used\",\n",
    "                            \"Year\": year.get_text(strip=True) if year else None,\n",
    "                            \"Make_Model\": make_model.get_text(strip=True) if make_model else None,\n",
    "                            \"Trim\": make_model.get_text(strip=True) if trim else None,\n",
    "                            \"Price\": price.get_text(strip=True) if price else None,\n",
    "                            \"Vin\": vin.get_text(strip=True) if vin else None,\n",
    "                            \"Date\": date.today()\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(\"Error parsing a card:\", e)\n",
    "\n",
    "            success = True\n",
    "            break  # Exit retry loop\n",
    "\n",
    "        except (InvalidSessionIdException, WebDriverException) as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Restarting browser...\")\n",
    "            driver = start_driver()\n",
    "            retries += 1\n",
    "            time.sleep(3)\n",
    "    if stop_scraping:\n",
    "        break  # Exit outer while True loop\n",
    "    if not success:\n",
    "        print(f\"Skipping page {page} after {max_retries} retries.\")\n",
    "        page += 1\n",
    "        continue\n",
    "\n",
    "    if not vehicle_cards:\n",
    "        break  # No more vehicles; end scraping\n",
    "\n",
    "    page += 1\n",
    "\n",
    "# Quit browser and save data\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = pd.DataFrame(vehicles)\n",
    "df.to_csv(\"selenium_LKNinf_paginated_scrape.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} vehicles across {page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c0295a-90f6-4332-b195-fa65ddfe2c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition     57\n",
       "Year          57\n",
       "Make_Model    57\n",
       "Trim          57\n",
       "Price          0\n",
       "Vin           57\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa2a46-2a39-4d16-b948-e3d0eaa4f287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
